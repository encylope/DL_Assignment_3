# -*- coding: utf-8 -*-
"""Untitled19 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VRexF_NvoHnKvfMyXUdaMGi89PXwD3NP

## NA21B075_DL_Assignment_3

Installing required libraries and setting up the environment for NLP and deep learning tasks
"""

!pip install wandb
!pip install wordcloud
!pip install colour
## Installing font for Hindi for matplotlib ##
!apt-get install -y fonts-lohit-deva
!fc-list :lang=hi family
import os
import random
import time
import wandb
import re, string
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud, STOPWORDS
from colour import Color
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from matplotlib.font_manager import FontProperties
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras import layers

from google.colab import drive
drive.mount('/content/drive')

import tarfile

tar_path = "/content/drive/MyDrive/dakshina_dataset_v1.0.tar"
extract_path = "/content/drive/MyDrive/dakshina_dataset_v1.0"

# Extract only if not already extracted
if not os.path.exists(extract_path):
    with tarfile.open(tar_path, 'r') as tar:
        tar.extractall(path="/content/drive/MyDrive")

def creating_data(language, path="/content/drive/MyDrive/dakshina_dataset_v1.0/{}/lexicons/{}.translit.sampled.{}.tsv"):
    # returning train tsv, val tsv, test tsv
    return path.format(language, language, "train"), path.format(language, language, "dev"), path.format(language, language, "test")

## functions for preprocessing data ##

def add_start_end_tokens(df, cols, sos="\t", eos="\n"):
    def add_tokens(s):
        # \t = starting token
        # \n = ending token
        return sos + str(s) + eos
    for col in cols:
        df[col] = df[col].apply(add_tokens)

def tokenize(lang, tokenizer=None):
    if tokenizer is None:
        tokenizer = Tokenizer(char_level=True)
        tokenizer.fit_on_texts(lang)

        lang_tensor = tokenizer.texts_to_sequences(lang)
        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,padding='post')
    else:
        lang_tensor = tokenizer.texts_to_sequences(lang)
        lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,padding='post')
    return lang_tensor, tokenizer

def preprocess_data(fpath, input_lang_tokenizer=None, targ_lang_tokenizer=None):
    df = pd.read_csv(fpath, sep="\t", header=None)
    add_start_end_tokens(df, [0,1])# adding start and end tokens to know when to stop predicting
    input_lang_tensor, input_tokenizer = tokenize(df[1].astype(str).tolist(),tokenizer=input_lang_tokenizer)
    targ_lang_tensor, targ_tokenizer = tokenize(df[0].astype(str).tolist(),tokenizer=targ_lang_tokenizer)
    dataset = tf.data.Dataset.from_tensor_slices((input_lang_tensor, targ_lang_tensor))
    dataset = dataset.shuffle(len(dataset))
    return dataset, input_tokenizer, targ_tokenizer

#########################  LAYER TYPE ##########################################
def model_layer_type(name, units, dropout, return_state=False, return_sequences=False):
    temp = layers.GRU(units=units, dropout=dropout, return_state=return_state, return_sequences=return_sequences)
    if name=="rnn":
      temp = layers.SimpleRNN(units=units, dropout=dropout, return_state=return_state, return_sequences=return_sequences)
    elif name == 'lstm':
      temp = layers.LSTM(units=units, dropout=dropout, return_state=return_state, return_sequences=return_sequences)
    return temp

def create_layer_for_Enc(no_of_layer, layer_type, units, dropout):
    temp = []
    for i in range(no_of_layer):
        ly = model_layer_type(layer_type, units, dropout, return_state=True, return_sequences=True)
        temp.append(ly)
    return temp


class Encoder(tf.keras.Model):
    def __init__(self, layer_type, n_layers, units, encoder_vocab_size, embedding_dim, dropout):
        super(Encoder, self).__init__()
        self.embedding = tf.keras.layers.Embedding(encoder_vocab_size, embedding_dim)
        self.dropout = dropout
        self.n_layers = n_layers
        self.units = units
        self.layer_type = layer_type
        self.rnn_layers = create_layer_for_Enc(n_layers, layer_type, units, dropout)

    def call(self, x, hidden):
        raise NotImplementedError("Use Enc_out_state() instead")

    def Enc_out_state(self, x, hidden):
        x = self.embedding(x)
        outputs = x

        for i in range(self.n_layers):
            if self.layer_type == "lstm":
                outputs, state_h, state_c = self.rnn_layers[i](outputs, initial_state=hidden)
                hidden = [state_h, state_c]
            else:
                outputs, state_h = self.rnn_layers[i](outputs, initial_state=hidden)
                hidden = [state_h]

        return outputs, hidden

    def initialize_hidden_state(self, batch_size):
        if self.layer_type == "lstm":
            return [tf.zeros((batch_size, self.units))] * 2
        else:
            return [tf.zeros((batch_size, self.units))]

def create_layer_for_Dec(no_of_layer, layer_type, units, dropout):
    temp = []
    for i in range(no_of_layer):
        # Only the last decoder layer returns a single timestep (return_sequences=False)
        return_seq = i < no_of_layer - 1
        ly = model_layer_type(layer_type, units, dropout, return_sequences=return_seq, return_state=True)
        temp.append(ly)
    return temp

class Decoder(tf.keras.Model):
    def __init__(self, type_of_layer, Total_layers, units, vocab_size, embed_dim, dropout, attention=False):
        super(Decoder, self).__init__()
        self.dense = layers.Dense(vocab_size, activation="softmax")
        self.n_layers = Total_layers
        self.units = units
        self.layer_type = type_of_layer
        self.dropout = dropout
        self.flatten = layers.Flatten()
        self.rnn_layers = create_layer_for_Dec(self.n_layers, self.layer_type, self.units, self.dropout)
        self.embed_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.attention = attention
        if self.attention:
            self.attention_layer = BahdanauAttention(self.units)

    def call(self, x, hidden, enc_out=None):
        raise NotImplementedError("Use Dec_pred_state() instead.")

    def Dec_pred_state(self, x, hidden, enc_out=None):
        x = self.embed_layer(x)

        if self.attention:
            context_vector, attention_weights = self.attention_layer(hidden, enc_out)
            x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)
        else:
            attention_weights = None

        outputs = x
        for i in range(self.n_layers):
            if self.layer_type == "lstm":
                outputs, state_h, state_c = self.rnn_layers[i](outputs, initial_state=hidden)
                hidden = [state_h, state_c]
            else:
                outputs, state_h = self.rnn_layers[i](outputs, initial_state=hidden)
                hidden = [state_h]

        preds = self.dense(self.flatten(outputs))
        return preds, hidden, attention_weights

class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, dec_hidden_state, enc_out):
        # If dec_hidden_state is a list (e.g., [h, c] for LSTM), use only h
        if isinstance(dec_hidden_state, list):
            dec_hidden_state = dec_hidden_state[0]

        # Shape: (batch_size, 1, units)
        dec_hidden_state = tf.expand_dims(dec_hidden_state, 1)

        # Shape: (batch_size, max_seq_len, 1)
        score = self.V(tf.nn.tanh(self.W1(dec_hidden_state) + self.W2(enc_out)))

        # Attention weights across encoder time steps
        attention_weights = tf.nn.softmax(score, axis=1)

        # Weighted sum of encoder outputs
        context_vector = attention_weights * enc_out
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

class BeamSearch():
    def __init__(self, model, k):
        self.k = k
        self.model = model
        self.acc = tf.keras.metrics.Accuracy()

    def sample_beam_search(self, probs):
        m, n = probs.shape
        output_sequences = [[[], 0.0]]
        for row in probs:
            beams = []
            for seq, score in output_sequences:
                for j in range(n):
                    new_seq = seq + [j]
                    new_score = score - tf.math.log(row[j])
                    beams.append([new_seq, new_score])
            output_sequences = sorted(beams, key=lambda x: x[1])[:self.k]

        tensors, scores = list(zip(*output_sequences))
        tensors = list(map(lambda x: tf.expand_dims(tf.constant(x), 0), tensors))
        return tf.concat(tensors, 0), scores

    def beam_accuracy(self, input, target):
        accs = []
        for i in range(self.k):
            self.acc.reset_state()
            self.acc.update_state(target, input[i, :])
            accs.append(self.acc.result())
        return max(accs)

    def step(self, input, target, enc_state):
        batch_acc = 0
        sequences = []

        enc_out, enc_state = self.model.encoder.Enc_out_state(input, enc_state)
        dec_state = enc_state
        dec_input = tf.expand_dims([self.model.targ_tokenizer.word_index["\t"]] * self.model.batch_size, 1)

        for t in range(1, target.shape[1]):
            preds, dec_state, _ = self.model.decoder.Dec_pred_state(dec_input, dec_state, enc_out)
            sequences.append(preds)
            preds = tf.argmax(preds, 1)
            dec_input = tf.expand_dims(preds, 1)

        sequences = tf.concat([tf.expand_dims(p, 1) for p in sequences], axis=1)

        for i in range(target.shape[0]):
            possibilities, scores = self.sample_beam_search(sequences[i, :, :])
            batch_acc += self.beam_accuracy(possibilities, target[i, 1:])

        return 0, batch_acc / target.shape[0]

    def evaluate(self, test_dataset, batch_size=None, upto=5, use_wandb=False):
        if batch_size is not None:
            self.model.batch_size = batch_size
            test_dataset = test_dataset.batch(batch_size)
        else:
            self.model.batch_size = 1

        test_acc = 0
        enc_state = self.model.encoder.initialize_hidden_state(self.model.batch_size)

        for batch, (input, target) in enumerate(test_dataset.take(upto)):
            _, acc = self.step(input, target, enc_state)
            test_acc += acc

        final_acc = test_acc / upto
        if use_wandb:
            wandb.log({"test acc (beam search)": final_acc})
        print(f"Test Accuracy on {upto * self.model.batch_size} samples: {final_acc:.4f}\n")

    def translate(self, word):
        word = "\t" + word + "\n"
        sequences = []

        inputs = self.model.input_tokenizer.texts_to_sequences([word])
        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,
                                                               maxlen=self.model.max_input_len,
                                                               padding="post")

        enc_state = self.model.encoder.initialize_hidden_state(1)
        enc_out, enc_state = self.model.encoder.Enc_out_state(inputs, enc_state)

        dec_state = enc_state
        dec_input = tf.expand_dims([self.model.targ_tokenizer.word_index["\t"]], 1)

        for t in range(1, self.model.max_target_len):
            preds, dec_state, _ = self.model.decoder.Dec_pred_state(dec_input, dec_state, enc_out)
            sequences.append(preds)
            preds = tf.argmax(preds, 1)
            dec_input = tf.expand_dims(preds, 1)

        sequences = tf.concat([tf.expand_dims(p, 1) for p in sequences], axis=1)

        possibilities, scores = self.sample_beam_search(tf.squeeze(sequences, 0))
        output_words = self.model.targ_tokenizer.sequences_to_texts(possibilities.numpy())

        def post_process(word):
            word = word.split(" ")[:-1]  # remove the <eos> token
            return "".join(word)

        output_words = list(map(post_process, output_words))
        return output_words, scores

class Seq2SeqModel():
    def __init__(self, embed_dim, enc_layers, dec_layers, type_layer, units, dropout, loss, optimizer, metric, attention=False):
        self.embed_dim = embed_dim
        self.enc_layers = enc_layers
        self.attention = attention
        self.dec_layers = dec_layers
        self.units = units
        self.dropout = dropout
        self.stats = []
        self.batch_size = 128
        self.type_layer = type_layer
        self.use_beam_search = False
        self.loss = loss
        self.optimizer = optimizer
        self.metric = metric

    def create_model(self):
        self.encoder = Encoder(self.type_layer, self.enc_layers, self.units, len(self.input_tokenizer.word_index) + 1, self.embed_dim, self.dropout)
        self.decoder = Decoder(self.type_layer, self.dec_layers, self.units, len(self.targ_tokenizer.word_index) + 1, self.embed_dim, self.dropout, self.attention)

    @tf.function
    def train_step(self, input, target, enc_state):
        loss = 0
        with tf.GradientTape() as tape:
            enc_out, enc_state = self.encoder.Enc_out_state(input, enc_state)
            dec_input = tf.expand_dims([self.targ_tokenizer.word_index["\t"]] * self.batch_size, 1)
            dec_state = enc_state
            for t in range(target.shape[1] - 1):
                x = t + 1
                preds, dec_state, _ = self.decoder.Dec_pred_state(dec_input, dec_state, enc_out)
                self.metric.update_state(target[:, x], preds)
                loss += self.loss(target[:, x], preds)
                if random.random() < self.teacher_forcing_ratio:
                    dec_input = tf.expand_dims(target[:, x], 1)
                else:
                    preds = tf.argmax(preds, 1)
                    dec_input = tf.expand_dims(preds, 1)

        variables = self.encoder.trainable_variables + self.decoder.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))
        return loss / target.shape[1], self.metric.result()

    @tf.function
    def validation_step(self, input, target, enc_state):
        loss = 0
        enc_out, enc_state = self.encoder.Enc_out_state(input, enc_state)
        dec_input = tf.expand_dims([self.targ_tokenizer.word_index["\t"]] * self.batch_size, 1)
        dec_state = enc_state
        for t in range(target.shape[1] - 1):
            preds, dec_state, _ = self.decoder.Dec_pred_state(dec_input, dec_state, enc_out)
            loss += self.loss(target[:, t + 1], preds)
            self.metric.update_state(target[:, t + 1], preds)
            preds = tf.argmax(preds, 1)
            dec_input = tf.expand_dims(preds, 1)
        return loss / target.shape[1], self.metric.result()

    def fit(self, dataset, validation_dataset, batch_size=128, epochs=5, use_wandb=False, teacher_forcing_ratio=1.0):
        self.batch_size = batch_size
        self.teacher_forcing_ratio = teacher_forcing_ratio
        dataset = dataset.batch(self.batch_size, drop_remainder=True)
        val_dataset = validation_dataset.batch(self.batch_size, drop_remainder=True)
        sample_inp, sample_targ = next(iter(dataset))
        self.max_input_len, self.max_target_len = sample_inp.shape[1], sample_targ.shape[1]

        for epoch in range(1, epochs + 1):
            print(f"Epoch {epoch}\nTraining ...")
            self.metric.reset_state()
            total_loss, total_acc = 0, 0
            enc_state = self.encoder.initialize_hidden_state(self.batch_size)

            for batch, (input, target) in enumerate(dataset):
                loss, acc = self.train_step(input, target, enc_state)
                total_loss += loss
                total_acc += acc

            avg_loss = total_loss / len(dataset)
            avg_acc = total_acc / len(dataset)

            print("Validating ...")
            self.metric.reset_state()
            total_val_loss, total_val_acc = 0, 0
            enc_state = self.encoder.initialize_hidden_state(self.batch_size)

            for batch, (input, target) in enumerate(val_dataset):
                loss, acc = self.validation_step(input, target, enc_state)
                total_val_loss += loss
                total_val_acc += acc

            val_loss = total_val_loss / len(val_dataset)
            val_acc = total_val_acc / len(val_dataset)

            epoch_stats = {
                "epoch": epoch,
                "train loss": avg_loss,
                "train acc": avg_acc * 100,
                "val loss": val_loss,
                "val acc": val_acc * 100,
                "training time": time.time()
            }

            if use_wandb:
                wandb.log(epoch_stats)
            self.stats.append(epoch_stats)

            print(f"Train Loss: {avg_loss:.4f}, Accuracy: {avg_acc*100:.2f}% | Val Loss: {val_loss:.4f}, Accuracy: {val_acc*100:.2f}%")

    def evaluate(self, test_dataset, batch_size=None):
        if batch_size is not None:
            self.batch_size = batch_size
        test_dataset = test_dataset.batch(self.batch_size, drop_remainder=True)

        self.metric.reset_state()
        total_loss, total_acc = 0, 0
        enc_state = self.encoder.initialize_hidden_state(self.batch_size)

        for batch, (input, target) in enumerate(test_dataset):
            loss, acc = self.validation_step(input, target, enc_state)
            total_loss += loss
            total_acc += acc

        print(f"Test Loss: {total_loss / len(test_dataset):.4f}, Test Accuracy: {total_acc / len(test_dataset):.4f}")
        return total_loss / len(test_dataset), total_acc / len(test_dataset)

    def translate(self, word, get_heatmap=False):
        word = "\t" + word + "\n"
        inputs = self.input_tokenizer.texts_to_sequences([word])
        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=self.max_input_len, padding="post")
        result = ""
        att_wts = []

        enc_state = self.encoder.initialize_hidden_state(1)
        enc_out, enc_state = self.encoder.Enc_out_state(inputs, enc_state)

        dec_state = enc_state
        dec_input = tf.expand_dims([self.targ_tokenizer.word_index["\t"]], 1)

        for t in range(1, self.max_target_len):
            preds, dec_state, attention_weights = self.decoder.Dec_pred_state(dec_input, dec_state, enc_out)
            if get_heatmap:
                att_wts.append(attention_weights)
            preds = tf.argmax(preds, 1)
            next_char = self.targ_tokenizer.index_word[preds.numpy().item()]
            result += next_char
            dec_input = tf.expand_dims(preds, 1)
            if next_char == "\n":
                return result[:-1], att_wts[:-1]
        return result[:-1], att_wts[:-1]

    def plot_attention_heatmap(self, word, ax, font_path="/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf"):
        translated_word, attn_wts = self.translate(word, get_heatmap=True)
        attn_heatmap = tf.squeeze(tf.concat(attn_wts, 0), -1).numpy()

        input_word_len = len(word)
        output_word_len = len(translated_word)

        ax.imshow(attn_heatmap[:, :input_word_len])
        font_prop = FontProperties(fname=font_path, size=18)

        ax.set_xticks(np.arange(input_word_len))
        ax.set_yticks(np.arange(output_word_len))

        ax.set_xticklabels(list(word))
        ax.set_yticklabels(list(translated_word), fontproperties=font_prop)

class Coloring():
    def __init__(self, word_color, color_default):
        self.word_color = word_color
        self.color_default = color_default
    def __call__(self, word, **kwargs):
        return self.word_color.get(word, self.color_default)

#get_color
def get_colors(inputs, targets, preds):
    n = len(targets)
    smoother = SmoothingFunction().method2
    def get_scores(target, output, smoother):
        return sentence_bleu([list(target)], list(output), smoothing_function=smoother)

    red = Color("red")
    colors = list(red.range_to(Color("violet"),n))
    colors = list(map(lambda c: c.hex, colors))
    scores = []
    for i in range(n):
        scores.append(get_scores(targets[i], preds[i], smoother))
    d = dict(zip(sorted(scores), list(range(n))))
    ordered_colors = list(map(lambda x: colors[d[x]], scores))
    input_colors = dict(zip(inputs, ordered_colors))
    target_colors = dict(zip(targets, ordered_colors))
    pred_colors = dict(zip(preds, ordered_colors))
    return input_colors, target_colors, pred_colors

#visualize_model_outputs
def visualize_model_outputs(model, test_file=creating_data("hi")[2], n=10, font_path="/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf"):
    df = pd.read_csv(test_file, sep="\t", header=None)
    df = df.sample(n=n).reset_index(drop=True)

    inputs = df[1].astype(str).tolist()
    targets = df[0].astype(str).tolist()
    preds = list(map(lambda word: model.translate(word)[0], inputs))
    input_colors, target_colors, pred_colors =  get_colors(inputs, targets, preds)# Generate colors for the words
    color_fn_ip = Coloring(input_colors, "white")
    color_fn_tr = Coloring(target_colors, "white")
    color_fn_op = Coloring(pred_colors, "white")
    input_text = Counter(inputs)
    target_text = Counter(targets)
    output_text = Counter(preds)
    fig, axs = plt.subplots(1,3, figsize=(30, 15))
    plt.tight_layout()
    wc_in = WordCloud(random_state=1).generate_from_frequencies(input_text)
    wc_out = WordCloud(font_path=font_path, random_state=1).generate_from_frequencies(output_text)
    wc_tar = WordCloud(font_path=font_path, random_state=1).generate_from_frequencies(target_text)
    axs[0].set_title("Input words", fontsize=30)
    axs[0].imshow(wc_in.recolor(color_func=color_fn_ip))
    axs[1].set_title("Target words", fontsize=30)
    axs[1].imshow(wc_tar.recolor(color_func=color_fn_tr))
    axs[2].set_title("Model Prediction Words", fontsize=30)
    axs[2].imshow(wc_out.recolor(color_func=color_fn_op))
    plt.show()

def Evaluating_Random_words(model, test_file=creating_data("hi")[2], n=10):

    df = pd.read_csv(test_file, sep="\t", header=None)
    df = df.sample(n=n).reset_index(drop=True)

    print(f"Arbitrarily evaluating the model on {n} words\n")
    for i in range(n):
        word = str(df[1][i])
        print(f"Input word: {word}")
        print(f"Actual translation of the word: {str(df[0][i])}")
        print(f"Model translation of the word: {model.translate(word)[0]}\n")

def Test_Model(lang, embed_dim, enc_layers, dec_layers, type_layer, units, dropout, attention, teacher_forcing_ratio=1.0, epch =20,save_outputs=None):
    TRAIN_TSV, VAL_TSV, TEST_TSV = creating_data(lang)
    model = Seq2SeqModel(embed_dim,enc_layers, dec_layers, type_layer,units,dropout, tf.keras.losses.SparseCategoricalCrossentropy(),tf.keras.optimizers.Adam(),tf.keras.metrics.SparseCategoricalAccuracy(),attention)
    dataset, input_tokenizer, targ_tokenizer = preprocess_data(TRAIN_TSV)
    val_dataset, _, _ = preprocess_data(VAL_TSV, input_tokenizer, targ_tokenizer)
    model.input_tokenizer = input_tokenizer
    model.targ_tokenizer = targ_tokenizer
    model.create_model()
    #model.fit(dataset, val_dataset, epochs=epch, use_wandb=False, teacher_forcing_ratio=teacher_forcing_ratio)
    model.fit(dataset, val_dataset, epochs=10, use_wandb=False, teacher_forcing_ratio=teacher_forcing_ratio)

    # Character accuracy
    test_dataset, _, _ = preprocess_data(TEST_TSV, model.input_tokenizer, model.targ_tokenizer)
    test_loss, test_acc = model.evaluate(test_dataset, batch_size=100)

    #  Word accuracy #
    test_tsv = pd.read_csv(TEST_TSV, sep="\t", header=None)
    inputs = test_tsv[1].astype(str).tolist()
    targets = test_tsv[0].astype(str).tolist()
    outputs = []

    for word in inputs:
        outputs.append(model.translate(word)[0])

    def word_level_acc(outputs, targets):
        return np.sum(np.asarray(outputs) == np.array(targets)) / len(outputs)

    word_acc = word_level_acc(outputs, targets)
    print(f"Word level accuracy: {word_acc}")

    #wandb.log({"Word-Accuracy":word_acc})

    #if save_outputs is None:
    df = pd.DataFrame()
    df["inputs"] = inputs
    df["targets"] = targets
    df["outputs"] = outputs
    df.to_csv('/content/drive/MyDrive/dakshina_dataset_v1.0/pred_attention.csv')
    return model

# Tools for getting model connectivity between input and output characters
def get_lstm_output(decoder, x, hidden, enc_out=None):
    x = decoder.embed_layer(x)

    if decoder.attention:
        context_vector, attention_weights = decoder.attention_layer(hidden, enc_out)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], -1)
    else:
        attention_weights = None

    outputs = x
    for i in range(len(decoder.rnn_layers)):
        if decoder.layer_type == "lstm":
            outputs, h, c = decoder.rnn_layers[i](outputs, initial_state=hidden)
            hidden = [h, c]
        else:
            outputs, h = decoder.rnn_layers[i](outputs, initial_state=hidden)
            hidden = [h]

    return outputs, hidden, attention_weights


def get_output_from_embedding(encoder, x, hidden):
    outputs = x
    for i in range(len(encoder.rnn_layers)):
        if encoder.layer_type == "lstm":
            outputs, h, c = encoder.rnn_layers[i](outputs, initial_state=hidden)
            hidden = [h, c]
        else:
            outputs, h = encoder.rnn_layers[i](outputs, initial_state=hidden)
            hidden = [h]
    return outputs, hidden


def get_connectivity(model, word):
    word = "\t" + word + "\n"
    inputs = model.input_tokenizer.texts_to_sequences([word])
    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,maxlen=model.max_input_len,padding="post")
    result = ""
    gradient_list = []
    enc_state = model.encoder.initialize_hidden_state(1)
    embedded_in = model.encoder.embedding(inputs)
    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:
        tape.watch(embedded_in)

        enc_out, enc_state = get_output_from_embedding(model.encoder, embedded_in, enc_state)

        dec_state = enc_state
        dec_input = tf.expand_dims([model.targ_tokenizer.word_index["\t"]]*1, 1)

        for t in range(1, model.max_target_len):

            lstm_out, dec_state, _ = get_lstm_output(model.decoder, dec_input, dec_state, enc_out)

            preds = model.decoder.dense(model.decoder.flatten(lstm_out))
            gradient_list.append(tape.gradient(lstm_out, embedded_in)[0])

            preds = tf.argmax(preds, 1)
            next_char = model.targ_tokenizer.index_word[preds.numpy().item()]
            result += next_char

            dec_input = tf.expand_dims(preds, 1)

            if next_char == "\n":
                return result[:-1], gradient_list[:-1]

        return result[:-1], gradient_list[:-1]

# Imports for visualising the model connectivity
from sklearn.preprocessing import MinMaxScaler
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from IPython.display import HTML as html_print
from IPython.display import display
import tensorflow.keras.backend as K

#html element
def cstr(s, color='black'):
    if s == ' ':
      return "<text style=color:#000;padding-left:10px;background-color:{}> </text>".format(color, s)
    else:
      return "<text style=color:#000;background-color:{}>{} </text>".format(color, s)

# print html
def print_color(t):
	  display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))

#appropriate color for value
def get_clr(value):
    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'
      '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',
      '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',
      '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']
    value = int(value * 19)
    if value == 19:
        value -= 1
    return colors[value]

# sigmoid function
def sigmoid(x):
    z = 1/(1 + np.exp(-x))
    return z

def softmax(x):
    v = np.exp(x)
    v = v / np.sum(v)
    return v

def get_gradient_norms(grad_list, word, activation="sigmoid"):
    grad_norms = []
    for grad_tensor in grad_list:
        grad_mags = tf.norm(grad_tensor, axis=1)
        grad_mags = grad_mags[:len(word)]
        if activation == "softmax":
            grad_mags_scaled = softmax(grad_mags)
        elif activation == "scaler":
            scaler = MinMaxScaler()
            grad_mags = tf.reshape(grad_mags, (-1,1))
            grad_mags_scaled = scaler.fit_transform(grad_mags)
        else:
            grad_mags_scaled = sigmoid(grad_mags)
        grad_norms.append(grad_mags_scaled)
    return grad_norms

def visualize(grad_norms, word, translated_word):
    print("Actual Word:", word)
    print("Transliterated Word:", translated_word)
    for i in range(len(translated_word)):
        print("Visualization for", translated_word[i],":")
        text_colours = []
        for j in range(len(grad_norms[i])):
            text = (word[j], get_clr(grad_norms[i][j]))
            text_colours.append(text)
        print_color(text_colours)

def visualise_connectivity(model, word, activation="sigmoid"):
    translated_word, grad_list = get_connectivity(model, word)
    grad_norms = get_gradient_norms(grad_list, word, activation)
    visualize(grad_norms, word, translated_word)

model = Test_Model(lang="hi",embed_dim=256,enc_layers=3,dec_layers=3,type_layer="lstm",units=256,dropout=0.2,attention=True)

visualize_model_outputs(model, n=10)

def get_test_words(n):
    test_df = pd.read_csv(creating_data("hi")[2])
    test_sample = test_df.sample(n)
    test_sample.reset_index(inplace=True, drop=True)
    test_words = []
    for i in test_sample.index:
        entry = test_sample["अंक\tank\t5"].loc[i]
        parts = entry.split("\t")
        word = parts[1]
        test_words.append(word)
    return test_words

test_words = get_test_words(5)
print(test_words)

for word in test_words:
    visualise_connectivity(model, word, activation="scaler")

Evaluating_Random_words(model, n=5)

wandb.login()

def Train_Our_Model(lang, test_beam_search=False):

    config_defaults = {"embeded_dim": 64,
                       "Teacher_forcing_ratio": 1.0,
                       "type_of_Layer": "lstm",
                       "decorder_encoder_layers": 1,
                       "units": 128,
                       "dropout": 0.1,
                       "Attention": False,
                       "Beam_width": 3,
                       "epochs":15
                       }

    wandb.init(config=config_defaults, project="Assignment_3", resume=True)

    ## 1. SELECT lang ##
    TRAIN_TSV, VAL_TSV, TEST_TSV = creating_data(lang)


    ## 2. DATA PREPROCESSING ##
    dataframe = pd.read_csv(TRAIN_TSV, sep="\t", header=None)
    def add_tokens(s, sos="\t", eos="\n"):
        return sos + str(s) + eos
    cols = [0,1]
    for col in cols:
        dataframe[col] = dataframe[col].apply(add_tokens)

    tokenizer = None
    if tokenizer is None:
        tokenizer = Tokenizer(char_level=True)
        tokenizer.fit_on_texts(dataframe[1].astype(str).tolist())
    lang_tensor = tokenizer.texts_to_sequences(dataframe[1].astype(str).tolist())
    lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,padding='post')
    input_lang_tensor, input_tokenizer = lang_tensor, tokenizer

    tokenizer = None
    if tokenizer is None:
        tokenizer = Tokenizer(char_level=True)
        tokenizer.fit_on_texts(dataframe[0].astype(str).tolist())
    lang_tensor = tokenizer.texts_to_sequences(dataframe[0].astype(str).tolist())
    lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,padding='post')

    targ_lang_tensor, targ_tokenizer = lang_tensor, tokenizer
    dataset = tf.data.Dataset.from_tensor_slices((input_lang_tensor, targ_lang_tensor))
    dataset = dataset.shuffle(len(dataset))

    dataframe = pd.read_csv(VAL_TSV, sep="\t", header=None)
    def add_tokens(s, sos="\t", eos="\n"):
        return sos + str(s) + eos
    cols = [0,1]
    for col in cols:
        dataframe[col] = dataframe[col].apply(add_tokens)
    lang_tensor = tokenizer.texts_to_sequences(dataframe[1].astype(str).tolist())
    lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,padding='post')
    input_lang_tensor = lang_tensor

    lang_tensor = tokenizer.texts_to_sequences(dataframe[0].astype(str).tolist())
    lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(lang_tensor,padding='post')
    targ_lang_tensor = lang_tensor
    val_dataset = tf.data.Dataset.from_tensor_slices((input_lang_tensor, targ_lang_tensor))
    val_dataset = dataset.shuffle(len(val_dataset))

    ## 3. CREATING THE MODEL ##
    model = Seq2SeqModel(embed_dim=wandb.config.embeded_dim,enc_layers=wandb.config.decorder_encoder_layers,dec_layers=wandb.config.decorder_encoder_layers, type_layer=wandb.config.type_of_Layer, units=wandb.config.units,dropout=wandb.config.dropout,loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer = tf.keras.optimizers.Adam(),metric = tf.keras.metrics.SparseCategoricalAccuracy(),attention=wandb.config.Attention)
    ## 4. COMPILING THE MODEL
    model.input_tokenizer = input_tokenizer
    model.targ_tokenizer = targ_tokenizer
    model.create_model()


    ## 5. FITTING AND VALIDATING THE MODEL
    model.fit(dataset, val_dataset, epochs=wandb.config.epochs, use_wandb=True, teacher_forcing_ratio=wandb.config.Teacher_forcing_ratio)

    if test_beam_search:
        ## OPTIONAL :- Evaluate the dataset using beam search and without beam search
        val_dataset, _, _ = preprocess_data(val_dataset, model.input_tokenizer, model.targ_tokenizer)
        subset = val_dataset.take(500)

        # a) Without beam search
        _, test_acc_without = model.evaluate(subset, batch_size=100)
        wandb.log({"Test acc": test_acc_without})

        # b) With beam search
        beam_search = BeamSearch(model=model, k=wandb.config.beam_width)
        beam_search.evaluate(subset, batch_size=100, use_wandb=True)

sweep_config = {
  "name": "Sweep_Assignment3",
  "method": "random",
  "parameters": {
        "decorder_encoder_layers": {
           "values": [1, 2, 3]
        },
        "units": {
            "values": [64, 128, 256]
        },
        "type_of_Layer": {
            "values": ["gru", "rnn","lstm"]
        },
         "embeded_dim": {
            "values": [256,64, 128]
        },
        "dropout": {
            "values": [0.29, 0.37]
        },
        "Beam_width": {
            "values": [3, 7, 5]
        },
        "Teacher_forcing_ratio": {
            "values": [0.9, 0.5,0.2]
        },
        "Attention": {
            "values": [True,False]
        },
        "epochs":{
            "values":[10,20,30]
        }

    }
}

sweep_id = wandb.sweep(sweep_config, project="Assignment_3")

wandb.agent(sweep_id, function=lambda: Train_Our_Model("hi"))